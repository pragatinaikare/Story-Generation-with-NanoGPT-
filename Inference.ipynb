{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a00a7ab-bab9-40da-9fbf-b793e2c0db09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 28.02 million\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random \n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset \n",
    "import numpy as np\n",
    "import tiktoken \n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"7\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "context_length = 512 \n",
    "batch_size =16\n",
    "emb_size= 256\n",
    "num_heads = 8\n",
    "dff= 512\n",
    "decoder_blocks = N = 4\n",
    "vocab_size = tokenizer.n_vocab\n",
    "lr = 3e-4\n",
    "dropout = 0.2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, emb_size, vocab_size, context_length, device):\n",
    "        super().__init__()\n",
    "        self.emb_size=emb_size\n",
    "        self.vocab_size=vocab_size\n",
    "        self.context_length= context_length\n",
    "        self.token_embd= nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_embd= nn.Embedding(context_length, emb_size)\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input shape is {x.shape}\")\n",
    "        x = self.token_embd(x)\n",
    "        # print(f\"Token embedding shape is {x.shape}\")\n",
    "        pos = torch.arange(x.shape[-2]).unsqueeze(0).to(device)\n",
    "        # print(f\"Positional Input is {pos.shape}\")\n",
    "        # print(f\"Positional {pos}\")\n",
    "\n",
    "        pos_embd = self.pos_embd(pos)\n",
    "        # print(f\"Positional Output is {pos_embd.shape}\")\n",
    "        x= pos_embd+x\n",
    "        # print(f\"Final embd output is {x.shape}\")\n",
    "        \n",
    "        return x \n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(self, emb_size, head_dim,context_length,dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_size=emb_size\n",
    "        self.head_dim=head_dim\n",
    "        \n",
    "        self.Wq = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        self.Wk = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        self.Wv = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        \n",
    "        self.mask=torch.tril(torch.ones(context_length,context_length)).view(1,context_length,context_length).to(device)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "                                \n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        T= x.shape[-2]  \n",
    "        q = self.Wq(x)\n",
    "        k =  self.Wk(x)\n",
    "        v=  self.Wv(x)\n",
    "                                \n",
    "        att_weights = q @ k.transpose(1,2) / math.sqrt(self.head_dim)        \n",
    "        att_weights = att_weights.masked_fill( self.mask[:,:T,:T]==0,-float(\"inf\"))\n",
    "        att_weights = F.softmax(att_weights,dim = -1)\n",
    "        att_weights = self.drop(att_weights)\n",
    "        # print(f\"Attention Weights -  {att_weights.shape}\")\n",
    "        out = att_weights @ v\n",
    "        # print(f\"Output {out.shape}\")\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,emb_size,num_heads,context_length,dropout, device):\n",
    "        super().__init__() \n",
    "        self.heads = nn.ModuleList([SingleHead(emb_size,emb_size//num_heads,context_length,dropout=dropout, device=device) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(emb_size, emb_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = self.proj(x) \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return  x\n",
    "        \n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,emb_size,dff, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.fc1 = nn.Linear(emb_size, dff)\n",
    "        self.fc2 = nn.Linear(dff, emb_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.multihead = MultiHead(emb_size,num_heads,context_length,dropout, device)\n",
    "        self.feedforward = FeedForward(emb_size,dff, dropout, device)      \n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.multihead(x)\n",
    "        out = F.relu(self.ln1(out)+x)\n",
    "        out1 = self.feedforward(out)\n",
    "        out1 = F.relu(self.ln2(out1)+out)\n",
    "        return out1\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,N, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.decstack = nn.ModuleList([Decoder(emb_size,dff,num_heads,context_length,dropout, device) for _ in range(N)])\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for decoder in self.decstack:\n",
    "            x= decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,N, vocab_size, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embd =  Embedding(emb_size,vocab_size,context_length, device)\n",
    "        self.decoderstack = DecoderStack(emb_size,dff,num_heads,context_length,N, dropout, device)\n",
    "        self.out = nn.Linear(emb_size,vocab_size, device)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embd(x)\n",
    "        x= self.decoderstack(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x \n",
    "\n",
    "gpt =  GPT(emb_size,dff,num_heads,context_length,N, vocab_size, dropout, device)  \n",
    "\n",
    "gpt.load_state_dict(torch.load(\"gpt_model_19.pth\", map_location=device))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "gpt.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in gpt.parameters() if p.requires_grad)\n",
    "\n",
    "num_params_million = num_params / 1e6\n",
    "\n",
    "print(f\"Number of parameters in the model: {num_params_million:.2f} million\")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "def next_word_pred(start, gpt):\n",
    "    start = \"\\n<|startofstory|>\\n\"+start\n",
    "    start_tokens = tokenizer.encode(start)\n",
    "    inp = torch.tensor(start_tokens).unsqueeze(0).to(device)\n",
    "    while inp[0][-9:].tolist() != [198, 27, 91, 437, 1659, 13571, 91, 29, 198]:\n",
    "        with torch.inference_mode():\n",
    "            prob = gpt(inp[:,-context_length:].to(device))\n",
    "        probs = F.softmax(prob[:,-1,:],dim=-1)\n",
    "        next_token = torch.multinomial(probs,num_samples=1)\n",
    "        inp = torch.cat([inp, next_token], dim=-1)\n",
    "        tokenizer.decode(inp.tolist()[0])\n",
    "        decoded_string = tokenizer.decode(inp.tolist()[0])\n",
    "        decoded_string = decoded_string.replace('\\n<|startofstory|>\\n', '')\n",
    "        decoded_string = decoded_string.replace('\\n<|endofstory|>\\n', '')\n",
    "    return decoded_string\n",
    "\n",
    "out = next_word_pred('Once upon a time, there was a little boy named Tim.', gpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9dd4755-6137-4256-8d3b-dc8c1a80c61c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little boy named Tim. He was a big kids who loved to play outside. One day, Tim saw a little boy named Timmy's fur. Timmy looked sad and hungry. Timmy ran to his mom and asked, \"What are you doing?\"\n",
      "\n",
      "His mom said, \"I am looking for food, Timmy. Do not worry because this puddle is yummy. They get food for you to eat.\"\n",
      "\n",
      "After a while, Timmy's mom came into the kitchen. She took a plate of cake and tasted it right down. She ate it and cheese. Timmy was so happy and ate his meal! From then on, Timmy loved the busy balance his big sack with his friends.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ad822-e121-4efc-9e4d-42c885657657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
