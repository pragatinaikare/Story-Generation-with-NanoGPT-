{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2429a11-4bc2-4925-9028-e4cd08c9d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random \n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset \n",
    "import numpy as np\n",
    "import tiktoken \n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"6\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca0efc9-ce00-445f-aa02-087a5e2f6763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6049120-6375-4d53-95c9-ca026c5138bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2baa96-a83c-4a5f-b662-e9ca3b2f1469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "context_length = 512 \n",
    "batch_size =16\n",
    "emb_size= 256\n",
    "num_heads = 8\n",
    "dff= 512\n",
    "decoder_blocks = N = 4\n",
    "vocab_size = tokenizer.n_vocab\n",
    "lr = 3e-4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e81c93f-1e62-4773-a60c-3cae7df8b6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From 2.1 Million stories loading only 10 random stories at time. Appending start and end token before and after each sentence. The randomly selecting context_length size window \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,context_length,num_of_stories,batch_size,n_itr, start='\\n<|startofstory|>\\n', end='\\n<|endofstory|>\\n'):\n",
    "        self.dataset = dataset \n",
    "        self.context_length = context_length  \n",
    "        self.num_of_stories = num_of_stories  \n",
    "        self.batch_size = batch_size \n",
    "        self.length = len(dataset)  \n",
    "        self.start = start \n",
    "        self.end = end \n",
    "        self.n_itr = n_itr\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size*self.n_itr\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        n = list(np.random.randint(0,self.length,size= self.num_of_stories))\n",
    "        text = ''\n",
    "        \n",
    "        for i in n:\n",
    "            \n",
    "            text += self.start + self.dataset[i] + self.end \n",
    "        \n",
    "            \n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        while len(tokens) < 513:\n",
    "            print(\"\")\n",
    "            text += text\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "            \n",
    "        start = random.randint(0, len(tokens) - self.context_length-1) \n",
    "        inputs = tokens[start: start+self.context_length]\n",
    "        outputs = tokens[start+1: start+self.context_length+1]\n",
    "        inputs = torch.tensor(inputs, dtype = torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype = torch.long)\n",
    "        assert inputs.shape[-1] == self.context_length, f\"Input's last dimension must be {self.context_length}, but got {inputs.shape[-1]}\"\n",
    "        assert outputs.shape[-1] == self.context_length, f\"Output's last dimension must be {self.context_length}, but got {outputs.shape[-1]}\"\n",
    "\n",
    "        return inputs,outputs\n",
    "    \n",
    "train_dataset =CustomDataset(ds['train']['text'], context_length=context_length ,num_of_stories=200 ,batch_size = batch_size, n_itr =3000)\n",
    "\n",
    "val_dataset = CustomDataset(ds['validation']['text'], context_length=context_length ,num_of_stories=200 ,batch_size = batch_size, n_itr=200)\n",
    "# dataloader = DataLoader(ds1, batch_size=1,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848b7dc2-fe7c-4462-9342-4851269af3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c490cd-3905-4158-9b8d-fe12b046ccc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for batch in dataloader:\n",
    "#     break\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1eca2c9-2273-4742-822c-e16ba740d63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     inputs, outputs = batch  \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58990e1-8603-45fb-8601-4269a3895315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f9836e-0f5b-4ea0-bbaf-26f93bc26e3c",
   "metadata": {},
   "source": [
    "<!-- Model Input -> (Batch_size, number_of_tokens)<br>\n",
    "Emb output -> (Batch_size, number_of_tokens, emb_size)<br>\n",
    "Positional embedding output -> (Batch_size, number_of_tokens, emb_size)<br>\n",
    "Decoder Input --> (Batch_size, number_of_tokens, emb_size)  --> Emb output + Positional embedding output<br>\n",
    "single head output -> (Batch_size, number_of_tokens, emb_size/num_heads (aka head_dim))<br>\n",
    "dff -> (32,256,384)--> (32,256,1024) --> (32,256,384)<br>\n",
    "decoder output-> (Batch_size, number_of_tokens, emb_size)<br>\n",
    "Model_final_out - > (Batch_size, number_of_tokens, vocab_size)<br>\n",
    "Y_true --> (Batch_size, number_of_tokens)  --><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abbeb8-4f8c-428d-b351-0caec8b68f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc9833a-363f-49b6-9197-05c2afbd1ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, emb_size, vocab_size, context_length, device):\n",
    "        super().__init__()\n",
    "        self.emb_size=emb_size\n",
    "        self.vocab_size=vocab_size\n",
    "        self.context_length= context_length\n",
    "        self.token_embd= nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_embd= nn.Embedding(context_length, emb_size)\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input shape is {x.shape}\")\n",
    "        x = self.token_embd(x)\n",
    "        # print(f\"Token embedding shape is {x.shape}\")\n",
    "        pos = torch.arange(x.shape[-2]).unsqueeze(0).to(device)\n",
    "        # print(f\"Positional Input is {pos.shape}\")\n",
    "        # print(f\"Positional {pos}\")\n",
    "\n",
    "        pos_embd = self.pos_embd(pos)\n",
    "        # print(f\"Positional Output is {pos_embd.shape}\")\n",
    "        x= pos_embd+x\n",
    "        # print(f\"Final embd output is {x.shape}\")\n",
    "        \n",
    "        return x \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece40bd3-7d50-44ec-a18c-311f3b1d6d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obj = Embedding(emb_size,vocab_size,context_length, device)\n",
    "# embd_out = obj(inputs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f7850d6-0f20-46a4-a4f1-9b9393fa3fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(self, emb_size, head_dim,context_length,dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_size=emb_size\n",
    "        self.head_dim=head_dim\n",
    "        \n",
    "        self.Wq = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        self.Wk = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        self.Wv = nn.Linear(emb_size, head_dim,bias=False)\n",
    "        \n",
    "        self.mask=torch.tril(torch.ones(context_length,context_length)).view(1,context_length,context_length).to(device)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "                                \n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        T= x.shape[-2]  \n",
    "        q = self.Wq(x)\n",
    "        k =  self.Wk(x)\n",
    "        v=  self.Wv(x)\n",
    "                                \n",
    "        att_weights = q @ k.transpose(1,2) / math.sqrt(self.head_dim)        \n",
    "        att_weights = att_weights.masked_fill( self.mask[:,:T,:T]==0,-float(\"inf\"))\n",
    "        att_weights = F.softmax(att_weights,dim = -1)\n",
    "        att_weights = self.drop(att_weights)\n",
    "        # print(f\"Attention Weights -  {att_weights.shape}\")\n",
    "        out = att_weights @ v\n",
    "        # print(f\"Output {out.shape}\")\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b736e0-b008-4a4c-a365-2ddc1986b31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sh = SingleHead(emb_size,emb_size//num_heads,context_length,dropout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d34981-af96-46b7-bbe5-776740d43970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# head_output = sh(embd_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a847cd-bc01-42f9-aedb-4d9ae761bef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,emb_size,num_heads,context_length,dropout, device):\n",
    "        super().__init__() \n",
    "        self.heads = nn.ModuleList([SingleHead(emb_size,emb_size//num_heads,context_length,dropout=dropout, device=device) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(emb_size, emb_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = self.proj(x) \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return  x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df39df5-e9a5-4515-8d6e-c717a83372b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mh = MultiHead(emb_size,num_heads,context_length,dropout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632c888a-e522-4ad2-80f4-2eac6cb48d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multihead_output  = Mh(embd_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c0c4885-60a3-4315-9c6c-3d6843ca44d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,emb_size,dff, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.fc1 = nn.Linear(emb_size, dff)\n",
    "        self.fc2 = nn.Linear(dff, emb_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "# ff = FeedForward(emb_size,dff, dropout, device)      \n",
    "# feed_forward_out= ff(multihead_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb4c108-de80-4392-a435-cdb735f504b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.multihead = MultiHead(emb_size,num_heads,context_length,dropout, device)\n",
    "        self.feedforward = FeedForward(emb_size,dff, dropout, device)      \n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.multihead(x)\n",
    "        out = F.relu(self.ln1(out)+x)\n",
    "        out1 = self.feedforward(out)\n",
    "        out1 = F.relu(self.ln2(out1)+out)\n",
    "        return out1\n",
    "    \n",
    "# decoder = Decoder(emb_size,dff,num_heads,context_length,dropout, device)\n",
    "# decoder_out = decoder(embd_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf63dcb8-b28b-46d7-b137-ac2cc3e4b779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,N, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.decstack = nn.ModuleList([Decoder(emb_size,dff,num_heads,context_length,dropout, device) for _ in range(N)])\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for decoder in self.decstack:\n",
    "            x= decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# ds = DecoderStack(emb_size,dff,num_heads,context_length,N, dropout, device)\n",
    "# ds_out = ds(embd_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bf3a5f-181c-4f09-bfab-2c57b0785e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Main Model _Classification\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,emb_size,dff,num_heads,context_length,N, vocab_size, dropout, device):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embd =  Embedding(emb_size,vocab_size,context_length, device)\n",
    "        self.decoderstack = DecoderStack(emb_size,dff,num_heads,context_length,N, dropout, device)\n",
    "        self.out = nn.Linear(emb_size,vocab_size, device)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embd(x)\n",
    "        x= self.decoderstack(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x \n",
    "\n",
    "gpt =  GPT(emb_size,dff,num_heads,context_length,N, vocab_size, dropout, device)  \n",
    "# gpt_out = gpt(inputs.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed1a8d99-06a4-4b69-8b07-dd26e6f9177e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 28.02 million\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in gpt.parameters() if p.requires_grad)\n",
    "\n",
    "num_params_million = num_params / 1e6\n",
    "\n",
    "print(f\"Number of parameters in the model: {num_params_million:.2f} million\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8535bbd4-be32-40f3-9ee5-eee20595e758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(),lr=lr)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_loss(dataloader):\n",
    "    val_loss = 0\n",
    "    gpt.eval() \n",
    "    with torch.inference_mode():\n",
    "        for inputs, outputs in val_dataloader:\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            pred = gpt(inputs)\n",
    "            loss = loss_fn(pred.view(-1, vocab_size), outputs.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    gpt.train() \n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def training(n_epochs):\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for inputs, outputs in train_dataloader:\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            \n",
    "            pred = gpt(inputs)\n",
    "            loss = loss_fn(pred.view(-1, vocab_size), outputs.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Avg training loss for epoch {epoch + 1}: {avg_train_loss}\")\n",
    "        total_loss = 0\n",
    "        \n",
    "        val_loss = calculate_loss(val_dataloader)\n",
    "        print(f\"Avg validation loss after epoch {epoch + 1}: {val_loss}\")\n",
    "        model_save_path = f'gpt_model_{epoch}.pth'\n",
    "\n",
    "        torch.save(gpt.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75d126-3d0e-45c7-8714-195f14070a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [59:17<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg training loss for epoch 1: 3.57179763118426\n",
      "Avg validation loss after epoch 1: 2.9561573684215547\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3000/3000 [1:00:15<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg training loss for epoch 2: 2.8619314579963686\n",
      "Avg validation loss after epoch 2: 2.6150129997730254\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████▎    | 2642/3000 [54:26<07:23,  1.24s/it]"
     ]
    }
   ],
   "source": [
    "losses = training(n_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61155cec-2119-4da8-8f61-dc97233bbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Inference Mode\n",
    "def next_word_pred(start, max_length, gpt):\n",
    "    start_tokens = encode(start)\n",
    "    inp = torch.tensor(start_tokens).unsqueeze(0)\n",
    "    for _ in range(max_length):\n",
    "        # print(inp.shape)\n",
    "        prob = gpt(inp[:,-256:])\n",
    "        probs = F.softmax(prob[:,-1,:],dim=-1)\n",
    "        next_token = torch.multinomial(probs,num_samples=1)\n",
    "        inp = torch.cat([inp, next_token], dim=-1)\n",
    "        \n",
    "    return decode(inp.tolist()[0])\n",
    "\n",
    "out = next_word_pred('Once', 289, gpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
